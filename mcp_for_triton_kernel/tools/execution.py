"""Execution and validation tools for Triton kernels."""

import json
from typing import Optional, Any

from fastmcp import FastMCP

from ..utils.runner import TritonRunner


# Global runner instance (lazy initialization)
_runner: Optional[TritonRunner] = None


def get_runner() -> TritonRunner:
    """Get or create the global TritonRunner instance."""
    global _runner
    if _runner is None:
        _runner = TritonRunner()
    return _runner


def register_execution_tools(mcp: FastMCP) -> None:
    """Register execution and validation tools to the MCP server."""

    @mcp.tool()
    def check_gpu_status() -> str:
        """
        GPU ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        
        Triton ì»¤ë„ ì‹¤í–‰ ì „ì— GPU ê°€ìš©ì„±ì„ í™•ì¸í•˜ì„¸ìš”.
        
        Returns:
            GPU ìƒíƒœ ì •ë³´ (ê°€ìš©ì„±, ë””ë°”ì´ìŠ¤ëª…, ë©”ëª¨ë¦¬ ë“±)
        """
        runner = get_runner()
        
        if not runner.gpu_available:
            return """âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

Triton ì»¤ë„ ì‹¤í–‰ì—ëŠ” CUDA GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.
í˜„ì¬ í™˜ê²½ì—ì„œëŠ” ì½”ë“œ ì‘ì„±ë§Œ ê°€ëŠ¥í•˜ê³ , ì‹¤í–‰ì€ GPU í™˜ê²½ì—ì„œ í•´ì•¼ í•©ë‹ˆë‹¤.
"""
        
        try:
            import torch
            
            gpu_info = {
                "available": True,
                "device_name": runner.gpu_name,
                "device_count": torch.cuda.device_count(),
                "current_device": torch.cuda.current_device(),
                "memory_allocated": f"{torch.cuda.memory_allocated() / 1024**3:.2f} GB",
                "memory_reserved": f"{torch.cuda.memory_reserved() / 1024**3:.2f} GB",
                "max_memory": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB",
            }
            
            return f"""âœ… GPU ì‚¬ìš© ê°€ëŠ¥

ë””ë°”ì´ìŠ¤: {gpu_info['device_name']}
ë””ë°”ì´ìŠ¤ ìˆ˜: {gpu_info['device_count']}
í˜„ì¬ ë””ë°”ì´ìŠ¤: {gpu_info['current_device']}
í• ë‹¹ëœ ë©”ëª¨ë¦¬: {gpu_info['memory_allocated']}
ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {gpu_info['memory_reserved']}
ì´ ë©”ëª¨ë¦¬: {gpu_info['max_memory']}
"""
        except Exception as e:
            return f"GPU ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}"

    @mcp.tool()
    def run_triton_kernel(
        code: str,
        test_input_code: str,
        entry_function: str = "solve",
    ) -> str:
        """
        Triton ì»¤ë„ ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            code: Triton ì»¤ë„ê³¼ solve í•¨ìˆ˜ê°€ í¬í•¨ëœ ì „ì²´ Python ì½”ë“œ
            test_input_code: í…ŒìŠ¤íŠ¸ ì…ë ¥ì„ ìƒì„±í•˜ëŠ” Python ì½”ë“œ
                            ë³€ìˆ˜ 'args'ì™€ 'kwargs'ë¥¼ ì •ì˜í•´ì•¼ í•¨
                            ì˜ˆ: "args = [torch.randn(1024, device='cuda')]"
            entry_function: í˜¸ì¶œí•  í•¨ìˆ˜ ì´ë¦„ (ê¸°ë³¸ê°’: "solve")
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼ (ì„±ê³µ ì‹œ ì¶œë ¥ ì •ë³´, ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€)
        
        Example:
            code = '''
            import torch
            import triton
            import triton.language as tl
            
            @triton.jit
            def add_kernel(a_ptr, b_ptr, c_ptr, N, BLOCK: tl.constexpr):
                idx = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)
                mask = idx < N
                a = tl.load(a_ptr + idx, mask=mask)
                b = tl.load(b_ptr + idx, mask=mask)
                tl.store(c_ptr + idx, a + b, mask=mask)
            
            def solve(a, b):
                c = torch.empty_like(a)
                N = a.numel()
                grid = lambda meta: (triton.cdiv(N, meta["BLOCK"]),)
                add_kernel[grid](a, b, c, N, BLOCK=256)
                return c
            '''
            
            test_input_code = '''
            import torch
            a = torch.randn(1024, device='cuda')
            b = torch.randn(1024, device='cuda')
            args = [a, b]
            kwargs = {}
            '''
        """
        runner = get_runner()
        
        if not runner.gpu_available:
            return """âŒ GPU ì—†ìŒ

GPUê°€ ì—†ì–´ì„œ ì»¤ë„ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
GPU í™˜ê²½ì—ì„œ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.

ì½”ë“œê°€ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ì§€ëŠ” í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
""" + _syntax_check(code)
        
        # Parse test inputs
        try:
            input_namespace = {}
            exec(test_input_code, input_namespace)
            args = input_namespace.get("args", [])
            kwargs = input_namespace.get("kwargs", {})
        except Exception as e:
            return f"""âŒ í…ŒìŠ¤íŠ¸ ì…ë ¥ ì½”ë“œ ì˜¤ë¥˜

{type(e).__name__}: {e}

test_input_codeëŠ” 'args'ì™€ 'kwargs' ë³€ìˆ˜ë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
ì˜ˆ:
```python
import torch
a = torch.randn(1024, device='cuda')
args = [a]
kwargs = {{}}
```
"""
        
        # Run kernel
        result = runner.execute_code(code, entry_function, args, kwargs)
        
        if result.success:
            output_info = _describe_output(result.output)
            return f"""âœ… ì‹¤í–‰ ì„±ê³µ

ì‹¤í–‰ ì‹œê°„: {result.execution_time_ms:.3f} ms

ì¶œë ¥:
{output_info}

stdout:
{result.stdout if result.stdout else "(ì—†ìŒ)"}
"""
        else:
            return f"""âŒ ì‹¤í–‰ ì‹¤íŒ¨

ì—ëŸ¬ íƒ€ì…: {result.error_type}
ì—ëŸ¬ ë©”ì‹œì§€: {result.error}

stderr:
{result.stderr if result.stderr else "(ì—†ìŒ)"}
"""

    @mcp.tool()
    def validate_correctness(
        kernel_code: str,
        reference_code: str,
        test_input_code: str,
        rtol: float = 1e-5,
        atol: float = 1e-8,
    ) -> str:
        """
        Triton ì»¤ë„ ì¶œë ¥ì„ PyTorch ì°¸ì¡° êµ¬í˜„ê³¼ ë¹„êµí•˜ì—¬ ì •í™•ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
        
        Args:
            kernel_code: Triton ì»¤ë„ ì½”ë“œ (solve í•¨ìˆ˜ í¬í•¨)
            reference_code: PyTorch ì°¸ì¡° êµ¬í˜„ ì½”ë“œ (reference í•¨ìˆ˜ í¬í•¨)
            test_input_code: í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„± ì½”ë“œ (args, kwargs ì •ì˜)
            rtol: ìƒëŒ€ í—ˆìš© ì˜¤ì°¨ (ê¸°ë³¸ê°’: 1e-5)
            atol: ì ˆëŒ€ í—ˆìš© ì˜¤ì°¨ (ê¸°ë³¸ê°’: 1e-8)
        
        Returns:
            ê²€ì¦ ê²°ê³¼ (í†µê³¼/ì‹¤íŒ¨, ì°¨ì´ í†µê³„)
        
        Example:
            kernel_code = '''
            # ... triton kernel code with solve() function ...
            '''
            
            reference_code = '''
            import torch
            def reference(a, b):
                return a + b
            '''
            
            test_input_code = '''
            import torch
            a = torch.randn(1024, device='cuda')
            b = torch.randn(1024, device='cuda')
            args = [a, b]
            kwargs = {}
            '''
        """
        runner = get_runner()
        
        if not runner.gpu_available:
            return "âŒ GPUê°€ ì—†ì–´ì„œ ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # Parse test inputs
        try:
            input_namespace = {}
            exec(test_input_code, input_namespace)
            args = input_namespace.get("args", [])
            kwargs = input_namespace.get("kwargs", {})
        except Exception as e:
            return f"âŒ í…ŒìŠ¤íŠ¸ ì…ë ¥ ì½”ë“œ ì˜¤ë¥˜: {e}"
        
        # Run triton kernel
        triton_result = runner.execute_code(kernel_code, "solve", args, kwargs)
        if not triton_result.success:
            return f"""âŒ Triton ì»¤ë„ ì‹¤í–‰ ì‹¤íŒ¨

ì—ëŸ¬: {triton_result.error}
{triton_result.stderr}
"""
        
        # Run reference implementation
        ref_result = runner.execute_code(reference_code, "reference", args, kwargs)
        if not ref_result.success:
            return f"""âŒ ì°¸ì¡° êµ¬í˜„ ì‹¤í–‰ ì‹¤íŒ¨

ì—ëŸ¬: {ref_result.error}
{ref_result.stderr}
"""
        
        # Validate
        validation = runner.validate_correctness(
            triton_result.output,
            ref_result.output,
            rtol=rtol,
            atol=atol,
        )
        
        if validation.error:
            return f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {validation.error}"
        
        if validation.passed:
            return f"""âœ… ê²€ì¦ í†µê³¼

ìµœëŒ€ ì°¨ì´: {validation.max_diff:.2e}
í‰ê·  ì°¨ì´: {validation.mean_diff:.2e}
ì „ì²´ ìš”ì†Œ: {validation.total_elements:,}
í—ˆìš© ì˜¤ì°¨: rtol={rtol}, atol={atol}
"""
        else:
            return f"""âŒ ê²€ì¦ ì‹¤íŒ¨

ìµœëŒ€ ì°¨ì´: {validation.max_diff:.2e}
í‰ê·  ì°¨ì´: {validation.mean_diff:.2e}
ë¶ˆì¼ì¹˜ ìš”ì†Œ: {validation.num_mismatches:,} / {validation.total_elements:,}
í—ˆìš© ì˜¤ì°¨: rtol={rtol}, atol={atol}

íŒ: fp16 ì‚¬ìš© ì‹œ rtol=1e-3, atol=1e-3 ì •ë„ê°€ ì ì ˆí•©ë‹ˆë‹¤.
"""

    @mcp.tool()
    def benchmark_kernel(
        kernel_code: str,
        test_input_code: str,
        reference_code: Optional[str] = None,
        warmup: int = 25,
        rep: int = 100,
    ) -> str:
        """
        Triton ì»¤ë„ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
        
        Args:
            kernel_code: Triton ì»¤ë„ ì½”ë“œ (solve í•¨ìˆ˜ í¬í•¨)
            test_input_code: í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„± ì½”ë“œ
            reference_code: (ì„ íƒ) ë¹„êµí•  PyTorch ì°¸ì¡° êµ¬í˜„
            warmup: ì›Œë°ì—… ì‹¤í–‰ íšŸìˆ˜ (ê¸°ë³¸ê°’: 25)
            rep: ì¸¡ì • ì‹¤í–‰ íšŸìˆ˜ (ê¸°ë³¸ê°’: 100)
        
        Returns:
            ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ (í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€ ì‹œê°„)
        """
        runner = get_runner()
        
        if not runner.gpu_available:
            return "âŒ GPUê°€ ì—†ì–´ì„œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # Parse test inputs
        try:
            input_namespace = {}
            exec(test_input_code, input_namespace)
            args = input_namespace.get("args", [])
            kwargs = input_namespace.get("kwargs", {})
        except Exception as e:
            return f"âŒ í…ŒìŠ¤íŠ¸ ì…ë ¥ ì½”ë“œ ì˜¤ë¥˜: {e}"
        
        # Get reference function if provided
        reference_fn = None
        if reference_code:
            try:
                ref_namespace = {}
                exec(reference_code, ref_namespace)
                reference_fn = ref_namespace.get("reference")
            except Exception as e:
                return f"âŒ ì°¸ì¡° ì½”ë“œ ì˜¤ë¥˜: {e}"
        
        # Run benchmark
        result = runner.benchmark(
            kernel_code,
            "solve",
            args,
            kwargs,
            warmup=warmup,
            rep=rep,
            reference_fn=reference_fn,
        )
        
        if not result.success:
            return f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {result.error}"
        
        output = f"""ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

ì‹¤í–‰ íšŸìˆ˜: {result.num_runs}
í‰ê· : {result.mean_ms:.4f} ms
í‘œì¤€í¸ì°¨: {result.std_ms:.4f} ms
ìµœì†Œ: {result.min_ms:.4f} ms
ìµœëŒ€: {result.max_ms:.4f} ms
"""
        
        if result.comparison:
            speedup = result.comparison.get("speedup", 0)
            ref_mean = result.comparison.get("reference_mean_ms", 0)
            
            if speedup >= 1:
                comparison_text = f"ğŸš€ Tritonì´ {speedup:.2f}x ë¹ ë¦„"
            else:
                comparison_text = f"âš ï¸ PyTorchê°€ {1/speedup:.2f}x ë¹ ë¦„"
            
            output += f"""
PyTorch ì°¸ì¡°: {ref_mean:.4f} ms
{comparison_text}
"""
        
        return output


def _syntax_check(code: str) -> str:
    """Check Python syntax without executing."""
    try:
        compile(code, "<string>", "exec")
        return "âœ… ë¬¸ë²• ê²€ì‚¬ í†µê³¼"
    except SyntaxError as e:
        return f"âŒ ë¬¸ë²• ì˜¤ë¥˜ (ë¼ì¸ {e.lineno}): {e.msg}"


def _describe_output(output: Any) -> str:
    """Describe the output tensor/value."""
    try:
        import torch
        
        if isinstance(output, torch.Tensor):
            return f"""Tensor:
  shape: {list(output.shape)}
  dtype: {output.dtype}
  device: {output.device}
  min: {output.min().item():.6f}
  max: {output.max().item():.6f}
  mean: {output.mean().item():.6f}"""
        elif isinstance(output, (list, tuple)):
            return f"{type(output).__name__} with {len(output)} elements"
        else:
            return str(output)[:500]
    except Exception as e:
        return f"(ì¶œë ¥ ì„¤ëª… ë¶ˆê°€: {e})"


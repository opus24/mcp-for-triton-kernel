"""Information tools for Triton kernel development."""

import json
from typing import Optional

from fastmcp import FastMCP

from ..knowledge import KNOWLEDGE_DIR, load_knowledge
from ..state import Status, get_state_manager, log_tool_call


def register_info_tools(mcp: FastMCP) -> None:
    """Register information-providing tools to the MCP server."""

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def get_overview() -> str:
        """
        Triton ì»¤ë„ ê°œë°œì˜ ì „ì²´ í”„ë¡œì„¸ìŠ¤ì™€ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ì»¤ë„ ê°œë°œì„ ì‹œì‘í•˜ê¸° ì „ì— ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì „ì²´ì ì¸ íë¦„ì„ íŒŒì•…í•˜ì„¸ìš”.

        Returns:
            Triton ì»¤ë„ ê°œë°œ ê°€ì´ë“œ ë¬¸ì„œ
        """
        state = get_state_manager()
        state.mark_info_collected("get_overview")

        content = load_knowledge("overview.md")

        status_hint = ""
        if state.can_transition_to_write():
            status_hint = "\n\nâœ… ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ! ìƒíƒœê°€ 'write'ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            missing = [t for t, done in state.info_collected.items() if not done]
            status_hint = f"\n\nğŸ“‹ ì•„ì§ ìˆ˜ì§‘ì´ í•„ìš”í•œ ì •ë³´: {', '.join(missing)}"

        return content + status_hint

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def get_triton_syntax() -> str:
        """
        Triton ë¬¸ë²•, tl í•¨ìˆ˜ë“¤, ì œì•½ì‚¬í•­ì— ëŒ€í•œ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ì»¤ë„ ì½”ë“œë¥¼ ì‘ì„±í•  ë•Œ ì°¸ê³ í•˜ì„¸ìš”.

        Returns:
            Triton ë¬¸ë²• ë ˆí¼ëŸ°ìŠ¤ ë¬¸ì„œ
        """
        state = get_state_manager()
        state.mark_info_collected("get_triton_syntax")

        content = load_knowledge("triton_syntax.md")

        status_hint = ""
        if state.can_transition_to_write():
            status_hint = "\n\nâœ… ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ! ìƒíƒœê°€ 'write'ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            missing = [t for t, done in state.info_collected.items() if not done]
            status_hint = f"\n\nğŸ“‹ ì•„ì§ ìˆ˜ì§‘ì´ í•„ìš”í•œ ì •ë³´: {', '.join(missing)}"

        return content + status_hint

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def get_torch_op_info(op_name: Optional[str] = None) -> str:
        """
        PyTorch ì—°ì‚°ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        íŠ¹ì • ì—°ì‚°ëª…ì„ ì§€ì •í•˜ë©´ í•´ë‹¹ ì—°ì‚°ì˜ ìƒì„¸ ì •ë³´ë¥¼,
        ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì§€ì›í•˜ëŠ” ëª¨ë“  ì—°ì‚° ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            op_name: ì¡°íšŒí•  ì—°ì‚° ì´ë¦„ (ì˜ˆ: "softmax", "matmul", "relu")
                    Noneì´ë©´ ì „ì²´ ëª©ë¡ ë°˜í™˜

        Returns:
            ì—°ì‚° ì •ë³´ (ì‹œê·¸ë‹ˆì²˜, ì„¤ëª…, Triton êµ¬í˜„ íŒ ë“±)
        """
        state = get_state_manager()
        state.mark_info_collected("get_torch_op_info")

        torch_ops_path = KNOWLEDGE_DIR / "torch_ops.json"

        if not torch_ops_path.exists():
            return "Error: torch_ops.json not found"

        with open(torch_ops_path, "r", encoding="utf-8") as f:
            ops_data = json.load(f)

        status_hint = ""
        if state.can_transition_to_write():
            status_hint = "\n\nâœ… ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ! ìƒíƒœê°€ 'write'ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            missing = [t for t, done in state.info_collected.items() if not done]
            status_hint = f"\n\nğŸ“‹ ì•„ì§ ìˆ˜ì§‘ì´ í•„ìš”í•œ ì •ë³´: {', '.join(missing)}"

        if op_name is None:
            # ì „ì²´ ëª©ë¡ ë°˜í™˜
            ops_list = list(ops_data.keys())
            return f"""ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ì‚° ëª©ë¡ ({len(ops_list)}ê°œ):

{chr(10).join(f"- {op}" for op in ops_list)}

íŠ¹ì • ì—°ì‚°ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë³´ë ¤ë©´ op_name ì¸ìë¥¼ ì§€ì •í•˜ì„¸ìš”.
ì˜ˆ: get_torch_op_info("softmax")
{status_hint}"""

        # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
        normalized_name = op_name.lower().strip()

        if normalized_name not in ops_data:
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            matches = [op for op in ops_data.keys() if normalized_name in op.lower()]
            if matches:
                return f"""'{op_name}' ì—°ì‚°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ìœ ì‚¬í•œ ì—°ì‚°:
{chr(10).join(f"- {m}" for m in matches)}
{status_hint}"""
            return f"'{op_name}' ì—°ì‚°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. get_torch_op_info()ë¡œ ì „ì²´ ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”.{status_hint}"

        op_info = ops_data[normalized_name]

        # ìµœì í™” ê¸°ë²• ì •ë³´ í¬ë§·íŒ…
        optimization_section = ""
        if "optimization_techniques" in op_info and op_info["optimization_techniques"]:
            techniques = op_info["optimization_techniques"]
            optimization_section = "\n## ğŸš€ ì¶”ì²œ ìµœì í™” ê¸°ë²• (4ê°€ì§€ ë²„ì „ ìƒì„±ìš©)\n\n"
            optimization_section += (
                "ë‹¤ìŒ 2ê°€ì§€ ìµœì í™” ê¸°ë²•ì„ ì¡°í•©í•˜ì—¬ v1~v4 ì»¤ë„ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n"
            )

            for i, tech in enumerate(techniques[:2], 1):  # ìµœëŒ€ 2ê°œë§Œ í‘œì‹œ
                optimization_section += f"### ê¸°ë²• {i}: {tech['name']}\n"
                optimization_section += f"{tech['description']}\n\n"

            optimization_section += "**4ê°€ì§€ ë²„ì „ êµ¬ì„±:**\n"
            optimization_section += "- **v1**: ê¸°ë³¸ êµ¬í˜„ (ìµœì í™” ì—†ìŒ)\n"
            optimization_section += f"- **v2**: {techniques[0]['name']}ë§Œ ì ìš©\n"
            if len(techniques) > 1:
                optimization_section += f"- **v3**: {techniques[1]['name']}ë§Œ ì ìš©\n"
                optimization_section += (
                    f"- **v4**: {techniques[0]['name']} + {techniques[1]['name']} ëª¨ë‘ ì ìš©\n"
                )
            else:
                optimization_section += f"- **v2~v4**: {techniques[0]['name']}ì˜ ë‹¤ì–‘í•œ ë³€í˜•\n"

        return f"""# {normalized_name}

## PyTorch ë™ë“± í‘œí˜„
{op_info.get('torch_equivalent', 'N/A')}

## ì‹œê·¸ë‹ˆì²˜
```python
{op_info.get('signature', 'N/A')}
```

## ì„¤ëª…
{op_info.get('description', 'N/A')}

## ì…ë ¥ Shape
{op_info.get('input_shapes', 'N/A')}

## ì¶œë ¥ Shape
{op_info.get('output_shape', 'N/A')}

## ë³µì¡ë„
{op_info.get('complexity', 'N/A')}

## ë©”ëª¨ë¦¬ íŒ¨í„´
{op_info.get('memory_pattern', 'N/A')}

## Triton êµ¬í˜„ íŒ
{op_info.get('triton_tips', 'N/A')}
{optimization_section}{status_hint}"""

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def get_kernel_template(pattern: str = "elementwise") -> str:
        """
        ì¼ë°˜ì ì¸ Triton ì»¤ë„ í…œí”Œë¦¿ì„ ì œê³µí•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        Args:
            pattern: ì»¤ë„ íŒ¨í„´ ì¢…ë¥˜
                - "elementwise": ìš”ì†Œë³„ ì—°ì‚° (add, mul, relu ë“±)
                - "reduction": ì¶•ì†Œ ì—°ì‚° (sum, mean, max ë“±)
                - "matmul": í–‰ë ¬ ê³±ì…ˆ
                - "fused": ìœµí•© ì»¤ë„ (ì˜ˆ: softmax)

        Returns:
            í•´ë‹¹ íŒ¨í„´ì˜ ì»¤ë„ í…œí”Œë¦¿ ì½”ë“œ
        """
        templates = {
            "elementwise": '''import torch
import triton
import triton.language as tl


@triton.jit
def elementwise_kernel(
    input_ptr,
    output_ptr,
    N,
    BLOCK_SIZE: tl.constexpr,
):
    """Element-wise operation kernel template."""
    pid = tl.program_id(0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < N

    # Load input
    x = tl.load(input_ptr + offsets, mask=mask)

    # TODO: ì—¬ê¸°ì— ì—°ì‚° êµ¬í˜„
    # ì˜ˆ: y = x * 2, y = tl.where(x > 0, x, 0), etc.
    y = x

    # Store output
    tl.store(output_ptr + offsets, y, mask=mask)


def solve(input: torch.Tensor) -> torch.Tensor:
    """Wrapper function to call the kernel."""
    output = torch.empty_like(input)
    N = input.numel()
    BLOCK_SIZE = 256

    grid = lambda meta: (triton.cdiv(N, meta["BLOCK_SIZE"]),)
    elementwise_kernel[grid](input, output, N, BLOCK_SIZE=BLOCK_SIZE)

    return output
''',
            "reduction": '''import torch
import triton
import triton.language as tl


@triton.jit
def reduction_kernel(
    input_ptr,
    output_ptr,
    M,  # number of rows
    N,  # number of columns (reduction dimension)
    stride_m,
    BLOCK_SIZE: tl.constexpr,
):
    """Row-wise reduction kernel template."""
    row_idx = tl.program_id(0)

    # Initialize accumulator
    acc = 0.0

    # Iterate over columns in blocks
    for start in range(0, N, BLOCK_SIZE):
        col_offsets = start + tl.arange(0, BLOCK_SIZE)
        mask = col_offsets < N

        # Load data
        ptrs = input_ptr + row_idx * stride_m + col_offsets
        x = tl.load(ptrs, mask=mask, other=0.0)

        # TODO: Accumulate (change operation as needed)
        acc += tl.sum(x, axis=0)

    # Store result
    tl.store(output_ptr + row_idx, acc)


def solve(input: torch.Tensor) -> torch.Tensor:
    """Wrapper function to call the kernel."""
    M, N = input.shape
    output = torch.empty(M, device=input.device, dtype=input.dtype)

    BLOCK_SIZE = min(triton.next_power_of_2(N), 1024)

    grid = (M,)
    reduction_kernel[grid](
        input, output, M, N, input.stride(0),
        BLOCK_SIZE=BLOCK_SIZE
    )

    return output
''',
            "matmul": '''import torch
import triton
import triton.language as tl


@triton.jit
def matmul_kernel(
    A_ptr, B_ptr, C_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Matrix multiplication kernel: C = A @ B."""
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    # Block starting positions
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Pointers to first block of A and B
    A_block_ptr = A_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    B_block_ptr = B_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn

    # Accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Loop over K dimension
    for k in range(0, K, BLOCK_K):
        k_mask = (k + offs_k) < K

        A_block = tl.load(A_block_ptr, mask=offs_m[:, None] < M and k_mask[None, :], other=0.0)
        B_block = tl.load(B_block_ptr, mask=k_mask[:, None] and offs_n[None, :] < N, other=0.0)

        acc += tl.dot(A_block, B_block)

        A_block_ptr += BLOCK_K * stride_ak
        B_block_ptr += BLOCK_K * stride_bk

    # Store result
    C_block_ptr = C_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(C_block_ptr, acc.to(C_ptr.dtype.element_ty), mask=mask)


def solve(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
    """Wrapper function to call the kernel."""
    M, K = A.shape
    K, N = B.shape
    C = torch.empty((M, N), device=A.device, dtype=A.dtype)

    BLOCK_M, BLOCK_N, BLOCK_K = 64, 64, 32

    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))
    matmul_kernel[grid](
        A, B, C,
        M, N, K,
        A.stride(0), A.stride(1),
        B.stride(0), B.stride(1),
        C.stride(0), C.stride(1),
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
    )

    return C
''',
            "fused": '''import torch
import triton
import triton.language as tl


@triton.jit
def fused_softmax_kernel(
    input_ptr,
    output_ptr,
    M,  # number of rows
    N,  # number of columns
    stride,
    BLOCK_SIZE: tl.constexpr,
):
    """Fused softmax kernel (numerically stable)."""
    row_idx = tl.program_id(0)
    row_start = input_ptr + row_idx * stride
    out_start = output_ptr + row_idx * stride

    # Load row
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < N
    x = tl.load(row_start + col_offsets, mask=mask, other=float("-inf"))

    # Compute softmax (numerically stable)
    x_max = tl.max(x, axis=0)
    x_shifted = x - x_max
    exp_x = tl.exp(x_shifted)
    sum_exp = tl.sum(exp_x, axis=0)
    softmax = exp_x / sum_exp

    # Store
    tl.store(out_start + col_offsets, softmax, mask=mask)


def solve(input: torch.Tensor) -> torch.Tensor:
    """Wrapper function to call the kernel."""
    M, N = input.shape
    output = torch.empty_like(input)

    # BLOCK_SIZE must be >= N for this simple version
    BLOCK_SIZE = triton.next_power_of_2(N)
    assert BLOCK_SIZE <= 2048, "Row too large for single-block softmax"

    grid = (M,)
    fused_softmax_kernel[grid](
        input, output, M, N, input.stride(0),
        BLOCK_SIZE=BLOCK_SIZE
    )

    return output
''',
        }

        if pattern not in templates:
            available = ", ".join(templates.keys())
            return f"Unknown pattern: {pattern}\nAvailable patterns: {available}"

        return templates[pattern]

"""Information tools for Triton kernel development."""

import json
from typing import Any, Dict, List, Optional

from fastmcp import FastMCP

from ..knowledge import KNOWLEDGE_DIR, load_knowledge
from ..state import Status, get_state_manager, log_tool_call

# ìµœì í™” ê¸°ë²• ì¹´íƒˆë¡œê·¸
OPTIMIZATION_CATALOG: List[Dict[str, Any]] = [
    {
        "name": "Online Reduction",
        "description": "Flash Attention ìŠ¤íƒ€ì¼ì˜ online max/sum ê³„ì‚°. ì—¬ëŸ¬ íŒ¨ìŠ¤ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì½ê¸° ìµœì†Œí™”. ìƒˆë¡œìš´ max ë°œê²¬ ì‹œ sum rescale.",
        "applies_to": ["softmax", "layernorm", "attention", "cross_entropy", "sum", "mean"],
        "keywords": ["reduction", "softmax", "max", "sum", "mean", "normalize"],
        "memory_patterns": ["row-wise reduction", "reduction"],
    },
    {
        "name": "Autotune",
        "description": "BLOCK_SIZEì™€ num_warpsë¥¼ ìë™ìœ¼ë¡œ íŠœë‹í•˜ì—¬ ë‹¤ì–‘í•œ ì…ë ¥ í¬ê¸°ì— ìµœì  ì„±ëŠ¥ ë‹¬ì„±.",
        "applies_to": ["all"],
        "keywords": ["any"],
        "memory_patterns": ["any"],
    },
    {
        "name": "Welford ì•Œê³ ë¦¬ì¦˜",
        "description": "Meanê³¼ Varianceë¥¼ í•œ ë²ˆì˜ íŒ¨ìŠ¤ë¡œ ë™ì‹œì— ê³„ì‚°. ë©”ëª¨ë¦¬ ì½ê¸° 2ë°° ê°ì†Œ.",
        "applies_to": ["layernorm", "batchnorm", "variance", "std"],
        "keywords": ["mean", "variance", "std", "normalize", "layer", "batch"],
        "memory_patterns": ["row-wise reduction", "reduction"],
    },
    {
        "name": "Tiled Processing",
        "description": "í° í–‰ë ¬ì„ ì‘ì€ íƒ€ì¼(BLOCK_M Ã— BLOCK_N Ã— BLOCK_K)ë¡œ ë‚˜ëˆ  ì²˜ë¦¬. ìºì‹œ íš¨ìœ¨ í–¥ìƒ, í° í–‰ë ¬ë„ ì²˜ë¦¬ ê°€ëŠ¥.",
        "applies_to": ["matmul", "attention", "conv"],
        "keywords": ["matmul", "matrix", "gemm", "attention", "conv"],
        "memory_patterns": ["íƒ€ì¼ë§", "shared memory", "2D"],
    },
    {
        "name": "Register Blocking",
        "description": "ì¤‘ê°„ ê²°ê³¼ë¥¼ ë ˆì§€ìŠ¤í„°ì— ìœ ì§€í•˜ì—¬ ë©”ëª¨ë¦¬ ì ‘ê·¼ ìµœì†Œí™”. ì—°ì‚° ê°•ë„ ì¦ê°€.",
        "applies_to": ["matmul", "attention", "conv"],
        "keywords": ["matmul", "matrix", "gemm", "attention"],
        "memory_patterns": ["íƒ€ì¼ë§", "shared memory"],
    },
    {
        "name": "Coalesced Memory Access",
        "description": "ì—°ì†ëœ ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ ìœ ì§€, stride ìµœì†Œí™”. ë©”ëª¨ë¦¬ bandwidth í™œìš© ê·¹ëŒ€í™”.",
        "applies_to": ["all"],
        "keywords": ["element", "vector", "add", "mul", "div"],
        "memory_patterns": ["element-wise", "ì™„ë²½í•˜ê²Œ ë³‘ë ¬í™”"],
    },
    {
        "name": "Kernel Specialization",
        "description": "tl.constexprë¡œ ì»´íŒŒì¼ íƒ€ì„ ìƒìˆ˜í™”, ì¡°ê±´ë¶€ ì»´íŒŒì¼. ë¶ˆí•„ìš”í•œ ë¶„ê¸° ì œê±°, ìµœì í™”ëœ ì½”ë“œ ìƒì„±.",
        "applies_to": ["dropout", "gelu", "cross_entropy"],
        "keywords": ["random", "activation", "special", "conditional"],
        "memory_patterns": ["element-wise with random", "element-wise"],
    },
]


def _analyze_operation(
    op_name: str, description: str = "", memory_pattern: str = ""
) -> List[Dict[str, Any]]:
    """
    ì—°ì‚° íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ì í•©í•œ ìµœì í™” ê¸°ë²• 2ê°œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

    Args:
        op_name: ì—°ì‚° ì´ë¦„
        description: ì—°ì‚° ì„¤ëª…
        memory_pattern: ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´

    Returns:
        ì¶”ì²œëœ ìµœì í™” ê¸°ë²• ë¦¬ìŠ¤íŠ¸ (2ê°œ)
    """
    scores: Dict[str, float] = {}
    op_name_lower = op_name.lower()
    description_lower = description.lower()
    memory_pattern_lower = memory_pattern.lower()

    for tech in OPTIMIZATION_CATALOG:
        score = 0.0
        tech_name = tech["name"]

        # 1. applies_to ë§¤ì¹­ (ë†’ì€ ê°€ì¤‘ì¹˜)
        if "all" in tech["applies_to"]:
            score += 1.0
        for applies in tech["applies_to"]:
            if applies.lower() in op_name_lower:
                score += 5.0

        # 2. í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in tech["keywords"]:
            if keyword == "any":
                score += 0.5
            elif keyword in op_name_lower:
                score += 3.0
            elif keyword in description_lower:
                score += 2.0

        # 3. ë©”ëª¨ë¦¬ íŒ¨í„´ ë§¤ì¹­ (ë†’ì€ ê°€ì¤‘ì¹˜)
        for pattern in tech["memory_patterns"]:
            if pattern == "any":
                score += 0.5
            elif pattern.lower() in memory_pattern_lower:
                score += 4.0

        scores[tech_name] = score

    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    sorted_techs = sorted(scores.items(), key=lambda x: x[1], reverse=True)

    # ìƒìœ„ 2ê°œ ì„ íƒ (Autotuneì´ í•­ìƒ í¬í•¨ë˜ë„ë¡ ë³´ì¥)
    selected = []
    autotune_tech = None

    for tech_name, score in sorted_techs:
        if tech_name == "Autotune":
            autotune_tech = next(t for t in OPTIMIZATION_CATALOG if t["name"] == tech_name)
        elif len(selected) < 1 and score > 0:
            tech = next(t for t in OPTIMIZATION_CATALOG if t["name"] == tech_name)
            selected.append(
                {
                    "name": tech["name"],
                    "description": tech["description"],
                    "applies_to": tech["applies_to"],
                }
            )

    # ì²« ë²ˆì§¸ê°€ ì—†ìœ¼ë©´ Coalesced Memory Access ì¶”ê°€
    if len(selected) == 0:
        coalesced = next(t for t in OPTIMIZATION_CATALOG if t["name"] == "Coalesced Memory Access")
        selected.append(
            {
                "name": coalesced["name"],
                "description": coalesced["description"],
                "applies_to": coalesced["applies_to"],
            }
        )

    # Autotune í•­ìƒ ë‘ ë²ˆì§¸ë¡œ ì¶”ê°€
    if autotune_tech:
        selected.append(
            {
                "name": autotune_tech["name"],
                "description": autotune_tech["description"],
                "applies_to": autotune_tech["applies_to"],
            }
        )

    return selected[:2]


def register_info_tools(mcp: FastMCP) -> None:
    """Register information-providing tools to the MCP server."""

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def get_overview() -> str:
        """
        Triton ì»¤ë„ ê°œë°œì˜ ì „ì²´ í”„ë¡œì„¸ìŠ¤ì™€ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ì»¤ë„ ê°œë°œì„ ì‹œì‘í•˜ê¸° ì „ì— ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì „ì²´ì ì¸ íë¦„ì„ íŒŒì•…í•˜ì„¸ìš”.

        Returns:
            Triton ì»¤ë„ ê°œë°œ ê°€ì´ë“œ ë¬¸ì„œ
        """
        state = get_state_manager()
        state.mark_info_collected("get_overview")

        content = load_knowledge("overview.md")

        status_hint = ""
        if state.can_transition_to_write():
            status_hint = "\n\nâœ… ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ! ìƒíƒœê°€ 'write'ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            missing = [t for t, done in state.info_collected.items() if not done]
            status_hint = f"\n\nğŸ“‹ ì•„ì§ ìˆ˜ì§‘ì´ í•„ìš”í•œ ì •ë³´: {', '.join(missing)}"

        return content + status_hint

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def get_triton_syntax() -> str:
        """
        Triton ë¬¸ë²•, tl í•¨ìˆ˜ë“¤, ì œì•½ì‚¬í•­ì— ëŒ€í•œ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ì»¤ë„ ì½”ë“œë¥¼ ì‘ì„±í•  ë•Œ ì°¸ê³ í•˜ì„¸ìš”.

        Returns:
            Triton ë¬¸ë²• ë ˆí¼ëŸ°ìŠ¤ ë¬¸ì„œ
        """
        state = get_state_manager()
        state.mark_info_collected("get_triton_syntax")

        content = load_knowledge("triton_syntax.md")

        status_hint = ""
        if state.can_transition_to_write():
            status_hint = "\n\nâœ… ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ! ìƒíƒœê°€ 'write'ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            missing = [t for t, done in state.info_collected.items() if not done]
            status_hint = f"\n\nğŸ“‹ ì•„ì§ ìˆ˜ì§‘ì´ í•„ìš”í•œ ì •ë³´: {', '.join(missing)}"

        return content + status_hint

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def get_torch_op_info(op_name: Optional[str] = None) -> str:
        """
        PyTorch ì—°ì‚°ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        íŠ¹ì • ì—°ì‚°ëª…ì„ ì§€ì •í•˜ë©´ í•´ë‹¹ ì—°ì‚°ì˜ ìƒì„¸ ì •ë³´ë¥¼,
        ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì§€ì›í•˜ëŠ” ëª¨ë“  ì—°ì‚° ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            op_name: ì¡°íšŒí•  ì—°ì‚° ì´ë¦„ (ì˜ˆ: "softmax", "matmul", "relu")
                    Noneì´ë©´ ì „ì²´ ëª©ë¡ ë°˜í™˜

        Returns:
            ì—°ì‚° ì •ë³´ (ì‹œê·¸ë‹ˆì²˜, ì„¤ëª…, Triton êµ¬í˜„ íŒ ë“±)
        """
        state = get_state_manager()
        state.mark_info_collected("get_torch_op_info")

        torch_ops_path = KNOWLEDGE_DIR / "torch_ops.json"

        if not torch_ops_path.exists():
            return "Error: torch_ops.json not found"

        with open(torch_ops_path, "r", encoding="utf-8") as f:
            ops_data = json.load(f)

        status_hint = ""
        if state.can_transition_to_write():
            status_hint = "\n\nâœ… ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ! ìƒíƒœê°€ 'write'ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            missing = [t for t, done in state.info_collected.items() if not done]
            status_hint = f"\n\nğŸ“‹ ì•„ì§ ìˆ˜ì§‘ì´ í•„ìš”í•œ ì •ë³´: {', '.join(missing)}"

        if op_name is None:
            # ì „ì²´ ëª©ë¡ ë°˜í™˜
            ops_list = list(ops_data.keys())
            return f"""ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ì‚° ëª©ë¡ ({len(ops_list)}ê°œ):

{chr(10).join(f"- {op}" for op in ops_list)}

íŠ¹ì • ì—°ì‚°ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë³´ë ¤ë©´ op_name ì¸ìë¥¼ ì§€ì •í•˜ì„¸ìš”.
ì˜ˆ: get_torch_op_info("softmax")
{status_hint}"""

        # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
        normalized_name = op_name.lower().strip()

        if normalized_name not in ops_data:
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            matches = [op for op in ops_data.keys() if normalized_name in op.lower()]
            if matches:
                return f"""'{op_name}' ì—°ì‚°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ìœ ì‚¬í•œ ì—°ì‚°:
{chr(10).join(f"- {m}" for m in matches)}
{status_hint}"""
            return f"'{op_name}' ì—°ì‚°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. get_torch_op_info()ë¡œ ì „ì²´ ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”.{status_hint}"

        op_info = ops_data[normalized_name]

        # ìµœì í™” ê¸°ë²• ì •ë³´ í¬ë§·íŒ…
        optimization_section = ""
        if "optimization_techniques" in op_info and op_info["optimization_techniques"]:
            techniques = op_info["optimization_techniques"]
            optimization_section = "\n## ğŸš€ ì¶”ì²œ ìµœì í™” ê¸°ë²• (4ê°€ì§€ ë²„ì „ ìƒì„±ìš©)\n\n"
            optimization_section += (
                "ë‹¤ìŒ 2ê°€ì§€ ìµœì í™” ê¸°ë²•ì„ ì¡°í•©í•˜ì—¬ v1~v4 ì»¤ë„ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n"
            )

            for i, tech in enumerate(techniques[:2], 1):  # ìµœëŒ€ 2ê°œë§Œ í‘œì‹œ
                optimization_section += f"### ê¸°ë²• {i}: {tech['name']}\n"
                optimization_section += f"{tech['description']}\n\n"

            optimization_section += "**4ê°€ì§€ ë²„ì „ êµ¬ì„±:**\n"
            optimization_section += "- **v1**: ê¸°ë³¸ êµ¬í˜„ (ìµœì í™” ì—†ìŒ)\n"
            optimization_section += f"- **v2**: {techniques[0]['name']}ë§Œ ì ìš©\n"
            if len(techniques) > 1:
                optimization_section += f"- **v3**: {techniques[1]['name']}ë§Œ ì ìš©\n"
                optimization_section += (
                    f"- **v4**: {techniques[0]['name']} + {techniques[1]['name']} ëª¨ë‘ ì ìš©\n"
                )
            else:
                optimization_section += f"- **v2~v4**: {techniques[0]['name']}ì˜ ë‹¤ì–‘í•œ ë³€í˜•\n"

        return f"""# {normalized_name}

## PyTorch ë™ë“± í‘œí˜„
{op_info.get('torch_equivalent', 'N/A')}

## ì‹œê·¸ë‹ˆì²˜
```python
{op_info.get('signature', 'N/A')}
```

## ì„¤ëª…
{op_info.get('description', 'N/A')}

## ì…ë ¥ Shape
{op_info.get('input_shapes', 'N/A')}

## ì¶œë ¥ Shape
{op_info.get('output_shape', 'N/A')}

## ë³µì¡ë„
{op_info.get('complexity', 'N/A')}

## ë©”ëª¨ë¦¬ íŒ¨í„´
{op_info.get('memory_pattern', 'N/A')}

## Triton êµ¬í˜„ íŒ
{op_info.get('triton_tips', 'N/A')}
{optimization_section}{status_hint}"""

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def get_kernel_template(pattern: str = "elementwise") -> str:
        """
        ì¼ë°˜ì ì¸ Triton ì»¤ë„ í…œí”Œë¦¿ì„ ì œê³µí•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        Args:
            pattern: ì»¤ë„ íŒ¨í„´ ì¢…ë¥˜
                - "elementwise": ìš”ì†Œë³„ ì—°ì‚° (add, mul, relu ë“±)
                - "reduction": ì¶•ì†Œ ì—°ì‚° (sum, mean, max ë“±)
                - "matmul": í–‰ë ¬ ê³±ì…ˆ
                - "fused": ìœµí•© ì»¤ë„ (ì˜ˆ: softmax)

        Returns:
            í•´ë‹¹ íŒ¨í„´ì˜ ì»¤ë„ í…œí”Œë¦¿ ì½”ë“œ
        """
        templates = {
            "elementwise": '''import torch
import triton
import triton.language as tl


@triton.jit
def elementwise_kernel(
    input_ptr,
    output_ptr,
    N,
    BLOCK_SIZE: tl.constexpr,
):
    """Element-wise operation kernel template."""
    pid = tl.program_id(0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < N

    # Load input
    x = tl.load(input_ptr + offsets, mask=mask)

    # TODO: ì—¬ê¸°ì— ì—°ì‚° êµ¬í˜„
    # ì˜ˆ: y = x * 2, y = tl.where(x > 0, x, 0), etc.
    y = x

    # Store output
    tl.store(output_ptr + offsets, y, mask=mask)


def solve(input: torch.Tensor) -> torch.Tensor:
    """Wrapper function to call the kernel."""
    output = torch.empty_like(input)
    N = input.numel()
    BLOCK_SIZE = 256

    grid = lambda meta: (triton.cdiv(N, meta["BLOCK_SIZE"]),)
    elementwise_kernel[grid](input, output, N, BLOCK_SIZE=BLOCK_SIZE)

    return output
''',
            "reduction": '''import torch
import triton
import triton.language as tl


@triton.jit
def reduction_kernel(
    input_ptr,
    output_ptr,
    M,  # number of rows
    N,  # number of columns (reduction dimension)
    stride_m,
    BLOCK_SIZE: tl.constexpr,
):
    """Row-wise reduction kernel template."""
    row_idx = tl.program_id(0)

    # Initialize accumulator
    acc = 0.0

    # Iterate over columns in blocks
    for start in range(0, N, BLOCK_SIZE):
        col_offsets = start + tl.arange(0, BLOCK_SIZE)
        mask = col_offsets < N

        # Load data
        ptrs = input_ptr + row_idx * stride_m + col_offsets
        x = tl.load(ptrs, mask=mask, other=0.0)

        # TODO: Accumulate (change operation as needed)
        acc += tl.sum(x, axis=0)

    # Store result
    tl.store(output_ptr + row_idx, acc)


def solve(input: torch.Tensor) -> torch.Tensor:
    """Wrapper function to call the kernel."""
    M, N = input.shape
    output = torch.empty(M, device=input.device, dtype=input.dtype)

    BLOCK_SIZE = min(triton.next_power_of_2(N), 1024)

    grid = (M,)
    reduction_kernel[grid](
        input, output, M, N, input.stride(0),
        BLOCK_SIZE=BLOCK_SIZE
    )

    return output
''',
            "matmul": '''import torch
import triton
import triton.language as tl


@triton.jit
def matmul_kernel(
    A_ptr, B_ptr, C_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Matrix multiplication kernel: C = A @ B."""
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    # Block starting positions
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Pointers to first block of A and B
    A_block_ptr = A_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    B_block_ptr = B_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn

    # Accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Loop over K dimension
    for k in range(0, K, BLOCK_K):
        k_mask = (k + offs_k) < K

        A_block = tl.load(A_block_ptr, mask=offs_m[:, None] < M and k_mask[None, :], other=0.0)
        B_block = tl.load(B_block_ptr, mask=k_mask[:, None] and offs_n[None, :] < N, other=0.0)

        acc += tl.dot(A_block, B_block)

        A_block_ptr += BLOCK_K * stride_ak
        B_block_ptr += BLOCK_K * stride_bk

    # Store result
    C_block_ptr = C_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(C_block_ptr, acc.to(C_ptr.dtype.element_ty), mask=mask)


def solve(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
    """Wrapper function to call the kernel."""
    M, K = A.shape
    K, N = B.shape
    C = torch.empty((M, N), device=A.device, dtype=A.dtype)

    BLOCK_M, BLOCK_N, BLOCK_K = 64, 64, 32

    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))
    matmul_kernel[grid](
        A, B, C,
        M, N, K,
        A.stride(0), A.stride(1),
        B.stride(0), B.stride(1),
        C.stride(0), C.stride(1),
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
    )

    return C
''',
            "fused": '''import torch
import triton
import triton.language as tl


@triton.jit
def fused_softmax_kernel(
    input_ptr,
    output_ptr,
    M,  # number of rows
    N,  # number of columns
    stride,
    BLOCK_SIZE: tl.constexpr,
):
    """Fused softmax kernel (numerically stable)."""
    row_idx = tl.program_id(0)
    row_start = input_ptr + row_idx * stride
    out_start = output_ptr + row_idx * stride

    # Load row
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < N
    x = tl.load(row_start + col_offsets, mask=mask, other=float("-inf"))

    # Compute softmax (numerically stable)
    x_max = tl.max(x, axis=0)
    x_shifted = x - x_max
    exp_x = tl.exp(x_shifted)
    sum_exp = tl.sum(exp_x, axis=0)
    softmax = exp_x / sum_exp

    # Store
    tl.store(out_start + col_offsets, softmax, mask=mask)


def solve(input: torch.Tensor) -> torch.Tensor:
    """Wrapper function to call the kernel."""
    M, N = input.shape
    output = torch.empty_like(input)

    # BLOCK_SIZE must be >= N for this simple version
    BLOCK_SIZE = triton.next_power_of_2(N)
    assert BLOCK_SIZE <= 2048, "Row too large for single-block softmax"

    grid = (M,)
    fused_softmax_kernel[grid](
        input, output, M, N, input.stride(0),
        BLOCK_SIZE=BLOCK_SIZE
    )

    return output
''',
        }

        if pattern not in templates:
            available = ", ".join(templates.keys())
            return f"Unknown pattern: {pattern}\nAvailable patterns: {available}"

        return templates[pattern]

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def analyze_and_save_optimization(
        op_name: str,
        torch_equivalent: str,
        signature: str,
        description: str,
        input_shapes: str,
        output_shape: str,
        complexity: str,
        memory_pattern: str,
        triton_tips: str,
    ) -> str:
        """
        ì—°ì‚°ì„ ë¶„ì„í•˜ì—¬ ì í•©í•œ ìµœì í™” ê¸°ë²• 2ê°œë¥¼ ì„ íƒí•˜ê³  torch_ops.jsonì— ì €ì¥í•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ìƒˆë¡œìš´ ì—°ì‚°ì„ ë“±ë¡í•˜ê±°ë‚˜ ê¸°ì¡´ ì—°ì‚°ì˜ ìµœì í™” ê¸°ë²•ì„ ì—…ë°ì´íŠ¸í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            op_name: ì—°ì‚° ì´ë¦„ (ì˜ˆ: "softmax", "matmul", "relu")
            torch_equivalent: PyTorch ë™ë“± í‘œí˜„ (ì˜ˆ: "torch.nn.functional.softmax(x, dim=-1)")
            signature: í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ (ì˜ˆ: "softmax(x: Tensor, dim: int = -1) -> Tensor")
            description: ì—°ì‚° ì„¤ëª… (ì˜ˆ: "exp(x - max(x)) / sum(exp(x - max(x)))")
            input_shapes: ì…ë ¥ shape ì •ë³´ (ì˜ˆ: "ë³´í†µ (batch, seq_len, hidden)")
            output_shape: ì¶œë ¥ shape ì •ë³´ (ì˜ˆ: "ì…ë ¥ê³¼ ë™ì¼")
            complexity: ë³µì¡ë„ (ì˜ˆ: "O(N) per row")
            memory_pattern: ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ (ì˜ˆ: "row-wise reduction í•„ìš”")
            triton_tips: Triton êµ¬í˜„ íŒ

        Returns:
            ë¶„ì„ ê²°ê³¼ ë° ì €ì¥ ê²°ê³¼
        """
        state = get_state_manager()

        # ì—°ì‚° ë¶„ì„í•˜ì—¬ ìµœì í™” ê¸°ë²• ì„ íƒ
        optimization_techniques = _analyze_operation(
            op_name=op_name,
            description=description,
            memory_pattern=memory_pattern,
        )

        # ìƒˆ ì—°ì‚° ë°ì´í„° êµ¬ì„±
        op_data = {
            "torch_equivalent": torch_equivalent,
            "signature": signature,
            "description": description,
            "input_shapes": input_shapes,
            "output_shape": output_shape,
            "complexity": complexity,
            "memory_pattern": memory_pattern,
            "triton_tips": triton_tips,
            "optimization_techniques": optimization_techniques,
        }

        # torch_ops.json ì½ê¸°
        torch_ops_path = KNOWLEDGE_DIR / "torch_ops.json"

        if torch_ops_path.exists():
            with open(torch_ops_path, "r", encoding="utf-8") as f:
                ops_data = json.load(f)
        else:
            ops_data = {}

        # ì—°ì‚° ì¶”ê°€/ì—…ë°ì´íŠ¸
        normalized_name = op_name.lower().strip()
        is_update = normalized_name in ops_data
        ops_data[normalized_name] = op_data

        # torch_ops.json ì €ì¥
        with open(torch_ops_path, "w", encoding="utf-8") as f:
            json.dump(ops_data, f, ensure_ascii=False, indent=2)

        # ìƒíƒœ íŒíŠ¸
        status_hint = ""
        if state.can_transition_to_write():
            status_hint = "\n\nâœ… ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ! ìƒíƒœê°€ 'write'ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            missing = [t for t, done in state.info_collected.items() if not done]
            status_hint = f"\n\nğŸ“‹ ì•„ì§ ìˆ˜ì§‘ì´ í•„ìš”í•œ ì •ë³´: {', '.join(missing)}"

        # ê²°ê³¼ í¬ë§·íŒ…
        action = "ì—…ë°ì´íŠ¸" if is_update else "ì¶”ê°€"
        techniques_str = "\n".join(
            [
                f"  {i+1}. **{tech['name']}**: {tech['description']}"
                for i, tech in enumerate(optimization_techniques)
            ]
        )

        return f"""# âœ… ì—°ì‚° ë¶„ì„ ì™„ë£Œ: {normalized_name}

## ğŸ“Š ë¶„ì„ ê²°ê³¼

### ì¶”ì²œ ìµœì í™” ê¸°ë²• (2ê°œ)
{techniques_str}

### 4ê°€ì§€ ë²„ì „ êµ¬ì„± ê°€ì´ë“œ
- **v1**: ê¸°ë³¸ êµ¬í˜„ (ìµœì í™” ì—†ìŒ)
- **v2**: {optimization_techniques[0]['name']}ë§Œ ì ìš©
- **v3**: {optimization_techniques[1]['name'] if len(optimization_techniques) > 1 else optimization_techniques[0]['name']}ë§Œ ì ìš©
- **v4**: {optimization_techniques[0]['name']} + {optimization_techniques[1]['name'] if len(optimization_techniques) > 1 else 'ì¶”ê°€ ìµœì í™”'} ëª¨ë‘ ì ìš©

## ğŸ’¾ ì €ì¥ ê²°ê³¼

- **íŒŒì¼**: `{torch_ops_path}`
- **ì‘ì—…**: {action}
- **ì—°ì‚°ëª…**: `{normalized_name}`
{status_hint}"""

    @mcp.tool()
    @log_tool_call(allowed_statuses=[Status.START])
    def get_optimization_catalog() -> str:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ìµœì í™” ê¸°ë²• ì¹´íƒˆë¡œê·¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        ì´ ë„êµ¬ëŠ” 'start' ìƒíƒœì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ì–´ë–¤ ìµœì í™” ê¸°ë²•ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Returns:
            ìµœì í™” ê¸°ë²• ëª©ë¡ ë° ì„¤ëª…
        """
        state = get_state_manager()

        catalog_str = ""
        for i, tech in enumerate(OPTIMIZATION_CATALOG, 1):
            applies_to = ", ".join(tech["applies_to"])
            catalog_str += f"""
### {i}. {tech["name"]}

**ì„¤ëª…**: {tech["description"]}

**ì ìš© ëŒ€ìƒ**: {applies_to}

---
"""

        status_hint = ""
        if state.can_transition_to_write():
            status_hint = "\n\nâœ… ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ! ìƒíƒœê°€ 'write'ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            missing = [t for t, done in state.info_collected.items() if not done]
            status_hint = f"\n\nğŸ“‹ ì•„ì§ ìˆ˜ì§‘ì´ í•„ìš”í•œ ì •ë³´: {', '.join(missing)}"

        return f"""# ğŸš€ Triton ì»¤ë„ ìµœì í™” ê¸°ë²• ì¹´íƒˆë¡œê·¸

ì´ {len(OPTIMIZATION_CATALOG)}ê°œì˜ ìµœì í™” ê¸°ë²•ì´ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
{catalog_str}

## ì‚¬ìš©ë²•

`analyze_and_save_optimization` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì—°ì‚° íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬
ìë™ìœ¼ë¡œ ì í•©í•œ ìµœì í™” ê¸°ë²• 2ê°œë¥¼ ì„ íƒí•˜ê³  `torch_ops.json`ì— ì €ì¥í•©ë‹ˆë‹¤.
{status_hint}"""
